{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445aff45",
   "metadata": {},
   "source": [
    "Task: Question Answering Based on Context\n",
    "\n",
    "### Sample Input\n",
    "\n",
    "- **Context:** \"The Eiffel Tower is located in Paris, France. It was completed in 1889 and stands at 330 meters tall.\"\n",
    "- **Question:** \"When was the Eiffel Tower completed?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd0ecf",
   "metadata": {},
   "source": [
    "Prompt Types and Simulated Outputs\n",
    "\n",
    "### 1. Direct Prompt\n",
    "\n",
    "**Prompt:**\n",
    "> Based on the following text, answer the question:  \n",
    "> \"The Eiffel Tower is located in Paris, France. It was completed in 1889 and stands at 330 meters tall.\"  \n",
    "> **Question:** When was the Eiffel Tower completed?  \n",
    "> **Answer:**\n",
    "\n",
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f53de",
   "metadata": {},
   "source": [
    "### 2. Few-Shot Prompt\n",
    "\n",
    "**Prompt:**\n",
    "> Read the passage and answer the question.\n",
    ">\n",
    "> **Example 1:**  \n",
    "> Passage: \"Mount Everest is the tallest mountain in the world.\"  \n",
    "> Question: What is Mount Everest known for?  \n",
    "> Answer: Being the tallest mountain in the world.\n",
    ">\n",
    "> **Example 2:**  \n",
    "> Passage: \"Barack Obama was the 44th President of the United States.\"  \n",
    "> Question: Who was Barack Obama?  \n",
    "> Answer: The 44th President of the United States.\n",
    ">\n",
    "> **Now your turn:**  \n",
    "> Passage: \"The Eiffel Tower is located in Paris, France. It was completed in 1889 and stands at 330 meters tall.\"  \n",
    "> Question: When was the Eiffel Tower completed?  \n",
    "> Answer:\n",
    "\n",
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87341171",
   "metadata": {},
   "source": [
    "### 3. Chain-of-Thought (CoT) Prompt\n",
    "\n",
    "**Prompt:**\n",
    "> Let's think step by step.  \n",
    "> Given the passage: \"The Eiffel Tower is located in Paris, France. It was completed in 1889 and stands at 330 meters tall.\"  \n",
    "> Question: When was the Eiffel Tower completed?  \n",
    "> Step-by-step reasoning:\n",
    "\n",
    "**Output:**\n",
    "> Step 1: Identify the relevant part of the passage.\n",
    "> Step 2: Look for information related to the completion date of the Eiffel Tower.\n",
    "> Step 3: The passage says it was completed in 1889.\n",
    "Final Answer: 1889\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc2d56",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Prompt Type | Output | Accuracy | Clarity | Reasoning Quality | Notes |\n",
    "|-------------|--------|----------|---------|-------------------|-------|\n",
    "| **Direct** | `1889` | High |  Low | None | Accurate but lacks explanation |\n",
    "| **Few-Shot** | `\"The Eiffel Tower was completed in 1889.\"` |  High |  Good |  Basic | Clear format, mimics examples |\n",
    "| **Chain-of-Thought** | Step-by-step + final answer | High | Good |  Strong | Shows internal logic, more transparent |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Among the three prompt styles tested:\n",
    "\n",
    "- **Direct prompt** provides a quick and correct response but offers no insight into how the model arrived at the answer.\n",
    "- **Few-shot prompt** improves consistency and clarity by leveraging example-based learning.\n",
    "- **Chain-of-thought (CoT) prompt** outperforms the others by encouraging logical reasoning and transparency in the model’s thinking process.\n",
    "\n",
    "**Conclusion:** While all prompts yield correct answers, the **chain-of-thought approach** delivers the best balance of accuracy, clarity, and explainability, making it ideal for complex or ambiguous tasks where understanding the model’s reasoning is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc64041",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
