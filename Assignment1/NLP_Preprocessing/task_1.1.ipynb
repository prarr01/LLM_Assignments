{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad5a51c",
   "metadata": {},
   "source": [
    "# Task 1.1: NLP Text Preprocessing Techniques\n",
    "\n",
    "In this notebook, we will implement and explore several fundamental NLP preprocessing techniques:\n",
    "1. Tokenization\n",
    "2. Lemmatization\n",
    "3. Stemming\n",
    "4. Part-of-Speech (POS) Tagging\n",
    "5. Named Entity Recognition (NER)\n",
    "\n",
    "We'll also compare lemmatization and stemming with at least 10 examples to understand their differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f81d03",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the necessary packages - NLTK and spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de78f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install nltk spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e127af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries and resources loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng',quiet=True)\n",
    "nltk.download('maxent_ne_chunker_tab',quiet=True)\n",
    "nltk.download('words',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"All libraries and resources loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3a1db",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into tokens (words, sentences, etc.). We'll demonstrate both sentence tokenization and word tokenization using NLTK and spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06afa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      "Natural Language Processing (NLP) is a subfield of artificial intelligence. It helps computers understand, interpret, and manipulate human language. The goal of NLP is to bridge the gap between human communication and computer understanding. Dr. Smith developed a new algorithm at Stanford University in California.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample text for all operations\n",
    "sample_text = \"\"\"Natural Language Processing (NLP) is a subfield of artificial intelligence. It helps computers understand, interpret, and manipulate human language. The goal of NLP is to bridge the gap between human communication and computer understanding. Dr. Smith developed a new algorithm at Stanford University in California.\"\"\"\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce6c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Sentence Tokenization:\n",
      "Sentence 1: Natural Language Processing (NLP) is a subfield of artificial intelligence.\n",
      "Sentence 2: It helps computers understand, interpret, and manipulate human language.\n",
      "Sentence 3: The goal of NLP is to bridge the gap between human communication and computer understanding.\n",
      "Sentence 4: Dr. Smith developed a new algorithm at Stanford University in California.\n",
      "\n",
      "NLTK Word Tokenization:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '.', 'It', 'helps', 'computers', 'understand', ',', 'interpret', ','] ...\n",
      "Total words: 53\n"
     ]
    }
   ],
   "source": [
    "# NLTK Tokenization\n",
    "print(\"NLTK Sentence Tokenization:\")\n",
    "sentences = sent_tokenize(sample_text)\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "\n",
    "print(\"\\nNLTK Word Tokenization:\")\n",
    "words = word_tokenize(sample_text)\n",
    "print(words[:20], \"...\")\n",
    "print(f\"Total words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425742b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Sentence Tokenization:\n",
      "Sentence 1: Natural Language Processing (NLP) is a subfield of artificial intelligence.\n",
      "Sentence 2: It helps computers understand, interpret, and manipulate human language.\n",
      "Sentence 3: The goal of NLP is to bridge the gap between human communication and computer understanding.\n",
      "Sentence 4: Dr. Smith developed a new algorithm at Stanford University in California.\n",
      "\n",
      "spaCy Word Tokenization:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '.', 'It', 'helps', 'computers', 'understand', ',', 'interpret', ','] ...\n",
      "Total tokens: 53\n",
      "\n",
      "Comparison of token count:\n",
      "NLTK: 53 tokens\n",
      "spaCy: 53 tokens\n",
      "\n",
      "Sentence 1: Natural Language Processing (NLP) is a subfield of artificial intelligence.\n",
      "Sentence 2: It helps computers understand, interpret, and manipulate human language.\n",
      "Sentence 3: The goal of NLP is to bridge the gap between human communication and computer understanding.\n",
      "Sentence 4: Dr. Smith developed a new algorithm at Stanford University in California.\n",
      "\n",
      "spaCy Word Tokenization:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '.', 'It', 'helps', 'computers', 'understand', ',', 'interpret', ','] ...\n",
      "Total tokens: 53\n",
      "\n",
      "Comparison of token count:\n",
      "NLTK: 53 tokens\n",
      "spaCy: 53 tokens\n"
     ]
    }
   ],
   "source": [
    "# spaCy Tokenization\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(\"spaCy Sentence Tokenization:\")\n",
    "for i, sent in enumerate(doc.sents, 1):\n",
    "    print(f\"Sentence {i}: {sent}\")\n",
    "\n",
    "print(\"\\nspaCy Word Tokenization:\")\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(spacy_tokens[:20], \"...\")\n",
    "print(f\"Total tokens: {len(spacy_tokens)}\")\n",
    "\n",
    "# Compare NLTK and spaCy tokenization\n",
    "print(\"\\nComparison of token count:\")\n",
    "print(f\"NLTK: {len(words)} tokens\")\n",
    "print(f\"spaCy: {len(spacy_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d35b6",
   "metadata": {},
   "source": [
    "## 2. Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form (known as lemma), considering the context and part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b94af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Lemmatization:\n",
      "is              -> be             \n",
      "helps           -> help           \n",
      "computers       -> computer       \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "# NLTK Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# We need POS information for better lemmatization with NLTK\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun\n",
    "\n",
    "# Lemmatize with POS information\n",
    "words_with_pos = pos_tag(words)\n",
    "lemmas_nltk = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in words_with_pos]\n",
    "\n",
    "print(\"NLTK Lemmatization:\")\n",
    "for original, lemma in list(zip(words, lemmas_nltk))[:20]:\n",
    "    if original != lemma:\n",
    "        print(f\"{original:<15} -> {lemma:<15}\")\n",
    "\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "772d6ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Lemmatization:\n",
      "is              -> be             \n",
      "It              -> it             \n",
      "helps           -> help           \n",
      "computers       -> computer       \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# spaCy Lemmatization\n",
    "lemmas_spacy = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"spaCy Lemmatization:\")\n",
    "for original, lemma in list(zip(spacy_tokens, lemmas_spacy))[:20]:\n",
    "    if original != lemma:\n",
    "        print(f\"{original:<15} -> {lemma:<15}\")\n",
    "\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa629e20",
   "metadata": {},
   "source": [
    "## 3. Stemming\n",
    "\n",
    "Stemming is the process of reducing words to their root/stem by removing affixes. Unlike lemmatization, stemming doesn't ensure that the resulting stem is a meaningful word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1139e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_be61d_row0_col0, #T_be61d_row0_col1, #T_be61d_row0_col2, #T_be61d_row0_col3, #T_be61d_row1_col0, #T_be61d_row1_col1, #T_be61d_row1_col2, #T_be61d_row1_col3, #T_be61d_row2_col0, #T_be61d_row2_col1, #T_be61d_row2_col2, #T_be61d_row2_col3, #T_be61d_row3_col0, #T_be61d_row3_col1, #T_be61d_row3_col2, #T_be61d_row3_col3, #T_be61d_row4_col0, #T_be61d_row4_col1, #T_be61d_row4_col2, #T_be61d_row4_col3, #T_be61d_row5_col0, #T_be61d_row5_col1, #T_be61d_row5_col2, #T_be61d_row5_col3, #T_be61d_row6_col0, #T_be61d_row6_col1, #T_be61d_row6_col2, #T_be61d_row6_col3, #T_be61d_row7_col0, #T_be61d_row7_col1, #T_be61d_row7_col2, #T_be61d_row7_col3, #T_be61d_row8_col0, #T_be61d_row8_col1, #T_be61d_row8_col2, #T_be61d_row8_col3, #T_be61d_row9_col0, #T_be61d_row9_col1, #T_be61d_row9_col2, #T_be61d_row9_col3, #T_be61d_row10_col0, #T_be61d_row10_col1, #T_be61d_row10_col2, #T_be61d_row10_col3, #T_be61d_row11_col0, #T_be61d_row11_col1, #T_be61d_row11_col2, #T_be61d_row11_col3, #T_be61d_row12_col0, #T_be61d_row12_col1, #T_be61d_row12_col2, #T_be61d_row12_col3, #T_be61d_row13_col0, #T_be61d_row13_col1, #T_be61d_row13_col2, #T_be61d_row13_col3, #T_be61d_row14_col0, #T_be61d_row14_col1, #T_be61d_row14_col2, #T_be61d_row14_col3, #T_be61d_row15_col0, #T_be61d_row15_col1, #T_be61d_row15_col2, #T_be61d_row15_col3, #T_be61d_row16_col0, #T_be61d_row16_col1, #T_be61d_row16_col2, #T_be61d_row16_col3, #T_be61d_row17_col0, #T_be61d_row17_col1, #T_be61d_row17_col2, #T_be61d_row17_col3, #T_be61d_row18_col0, #T_be61d_row18_col1, #T_be61d_row18_col2, #T_be61d_row18_col3, #T_be61d_row19_col0, #T_be61d_row19_col1, #T_be61d_row19_col2, #T_be61d_row19_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_be61d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_be61d_level0_col0\" class=\"col_heading level0 col0\" >Original</th>\n",
       "      <th id=\"T_be61d_level0_col1\" class=\"col_heading level0 col1\" >Porter Stemmer</th>\n",
       "      <th id=\"T_be61d_level0_col2\" class=\"col_heading level0 col2\" >Lancaster Stemmer</th>\n",
       "      <th id=\"T_be61d_level0_col3\" class=\"col_heading level0 col3\" >Snowball Stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_be61d_row0_col0\" class=\"data row0 col0\" >Natural</td>\n",
       "      <td id=\"T_be61d_row0_col1\" class=\"data row0 col1\" >natur</td>\n",
       "      <td id=\"T_be61d_row0_col2\" class=\"data row0 col2\" >nat</td>\n",
       "      <td id=\"T_be61d_row0_col3\" class=\"data row0 col3\" >natur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_be61d_row1_col0\" class=\"data row1 col0\" >Language</td>\n",
       "      <td id=\"T_be61d_row1_col1\" class=\"data row1 col1\" >languag</td>\n",
       "      <td id=\"T_be61d_row1_col2\" class=\"data row1 col2\" >langu</td>\n",
       "      <td id=\"T_be61d_row1_col3\" class=\"data row1 col3\" >languag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_be61d_row2_col0\" class=\"data row2 col0\" >Processing</td>\n",
       "      <td id=\"T_be61d_row2_col1\" class=\"data row2 col1\" >process</td>\n",
       "      <td id=\"T_be61d_row2_col2\" class=\"data row2 col2\" >process</td>\n",
       "      <td id=\"T_be61d_row2_col3\" class=\"data row2 col3\" >process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_be61d_row3_col0\" class=\"data row3 col0\" >(</td>\n",
       "      <td id=\"T_be61d_row3_col1\" class=\"data row3 col1\" >(</td>\n",
       "      <td id=\"T_be61d_row3_col2\" class=\"data row3 col2\" >(</td>\n",
       "      <td id=\"T_be61d_row3_col3\" class=\"data row3 col3\" >(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_be61d_row4_col0\" class=\"data row4 col0\" >NLP</td>\n",
       "      <td id=\"T_be61d_row4_col1\" class=\"data row4 col1\" >nlp</td>\n",
       "      <td id=\"T_be61d_row4_col2\" class=\"data row4 col2\" >nlp</td>\n",
       "      <td id=\"T_be61d_row4_col3\" class=\"data row4 col3\" >nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_be61d_row5_col0\" class=\"data row5 col0\" >)</td>\n",
       "      <td id=\"T_be61d_row5_col1\" class=\"data row5 col1\" >)</td>\n",
       "      <td id=\"T_be61d_row5_col2\" class=\"data row5 col2\" >)</td>\n",
       "      <td id=\"T_be61d_row5_col3\" class=\"data row5 col3\" >)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_be61d_row6_col0\" class=\"data row6 col0\" >is</td>\n",
       "      <td id=\"T_be61d_row6_col1\" class=\"data row6 col1\" >is</td>\n",
       "      <td id=\"T_be61d_row6_col2\" class=\"data row6 col2\" >is</td>\n",
       "      <td id=\"T_be61d_row6_col3\" class=\"data row6 col3\" >is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_be61d_row7_col0\" class=\"data row7 col0\" >a</td>\n",
       "      <td id=\"T_be61d_row7_col1\" class=\"data row7 col1\" >a</td>\n",
       "      <td id=\"T_be61d_row7_col2\" class=\"data row7 col2\" >a</td>\n",
       "      <td id=\"T_be61d_row7_col3\" class=\"data row7 col3\" >a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_be61d_row8_col0\" class=\"data row8 col0\" >subfield</td>\n",
       "      <td id=\"T_be61d_row8_col1\" class=\"data row8 col1\" >subfield</td>\n",
       "      <td id=\"T_be61d_row8_col2\" class=\"data row8 col2\" >subfield</td>\n",
       "      <td id=\"T_be61d_row8_col3\" class=\"data row8 col3\" >subfield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_be61d_row9_col0\" class=\"data row9 col0\" >of</td>\n",
       "      <td id=\"T_be61d_row9_col1\" class=\"data row9 col1\" >of</td>\n",
       "      <td id=\"T_be61d_row9_col2\" class=\"data row9 col2\" >of</td>\n",
       "      <td id=\"T_be61d_row9_col3\" class=\"data row9 col3\" >of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_be61d_row10_col0\" class=\"data row10 col0\" >artificial</td>\n",
       "      <td id=\"T_be61d_row10_col1\" class=\"data row10 col1\" >artifici</td>\n",
       "      <td id=\"T_be61d_row10_col2\" class=\"data row10 col2\" >art</td>\n",
       "      <td id=\"T_be61d_row10_col3\" class=\"data row10 col3\" >artifici</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_be61d_row11_col0\" class=\"data row11 col0\" >intelligence</td>\n",
       "      <td id=\"T_be61d_row11_col1\" class=\"data row11 col1\" >intellig</td>\n",
       "      <td id=\"T_be61d_row11_col2\" class=\"data row11 col2\" >intellig</td>\n",
       "      <td id=\"T_be61d_row11_col3\" class=\"data row11 col3\" >intellig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_be61d_row12_col0\" class=\"data row12 col0\" >.</td>\n",
       "      <td id=\"T_be61d_row12_col1\" class=\"data row12 col1\" >.</td>\n",
       "      <td id=\"T_be61d_row12_col2\" class=\"data row12 col2\" >.</td>\n",
       "      <td id=\"T_be61d_row12_col3\" class=\"data row12 col3\" >.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_be61d_row13_col0\" class=\"data row13 col0\" >It</td>\n",
       "      <td id=\"T_be61d_row13_col1\" class=\"data row13 col1\" >it</td>\n",
       "      <td id=\"T_be61d_row13_col2\" class=\"data row13 col2\" >it</td>\n",
       "      <td id=\"T_be61d_row13_col3\" class=\"data row13 col3\" >it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_be61d_row14_col0\" class=\"data row14 col0\" >helps</td>\n",
       "      <td id=\"T_be61d_row14_col1\" class=\"data row14 col1\" >help</td>\n",
       "      <td id=\"T_be61d_row14_col2\" class=\"data row14 col2\" >help</td>\n",
       "      <td id=\"T_be61d_row14_col3\" class=\"data row14 col3\" >help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_be61d_row15_col0\" class=\"data row15 col0\" >computers</td>\n",
       "      <td id=\"T_be61d_row15_col1\" class=\"data row15 col1\" >comput</td>\n",
       "      <td id=\"T_be61d_row15_col2\" class=\"data row15 col2\" >comput</td>\n",
       "      <td id=\"T_be61d_row15_col3\" class=\"data row15 col3\" >comput</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_be61d_row16_col0\" class=\"data row16 col0\" >understand</td>\n",
       "      <td id=\"T_be61d_row16_col1\" class=\"data row16 col1\" >understand</td>\n",
       "      <td id=\"T_be61d_row16_col2\" class=\"data row16 col2\" >understand</td>\n",
       "      <td id=\"T_be61d_row16_col3\" class=\"data row16 col3\" >understand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_be61d_row17_col0\" class=\"data row17 col0\" >,</td>\n",
       "      <td id=\"T_be61d_row17_col1\" class=\"data row17 col1\" >,</td>\n",
       "      <td id=\"T_be61d_row17_col2\" class=\"data row17 col2\" >,</td>\n",
       "      <td id=\"T_be61d_row17_col3\" class=\"data row17 col3\" >,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_be61d_row18_col0\" class=\"data row18 col0\" >interpret</td>\n",
       "      <td id=\"T_be61d_row18_col1\" class=\"data row18 col1\" >interpret</td>\n",
       "      <td id=\"T_be61d_row18_col2\" class=\"data row18 col2\" >interpret</td>\n",
       "      <td id=\"T_be61d_row18_col3\" class=\"data row18 col3\" >interpret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be61d_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_be61d_row19_col0\" class=\"data row19 col0\" >,</td>\n",
       "      <td id=\"T_be61d_row19_col1\" class=\"data row19 col1\" >,</td>\n",
       "      <td id=\"T_be61d_row19_col2\" class=\"data row19 col2\" >,</td>\n",
       "      <td id=\"T_be61d_row19_col3\" class=\"data row19 col3\" >,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1bc947e10f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Apply stemming\n",
    "stems_porter = [porter_stemmer.stem(word) for word in words]\n",
    "stems_lancaster = [lancaster_stemmer.stem(word) for word in words]\n",
    "stems_snowball = [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "stemming_df = pd.DataFrame({\n",
    "    'Original': words[:20],\n",
    "    'Porter Stemmer': stems_porter[:20],\n",
    "    'Lancaster Stemmer': stems_lancaster[:20],\n",
    "    'Snowball Stemmer': stems_snowball[:20]\n",
    "})\n",
    "\n",
    "print(\"Stemming Comparison:\")\n",
    "display(stemming_df.style.set_properties(**{'text-align': 'left'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac01ca0",
   "metadata": {},
   "source": [
    "## 4. Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging assigns grammatical categories (like noun, verb, adjective) to each word in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f235cdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK POS Tagging:\n",
      "Natural         -> JJ    \n",
      "Language        -> NNP   \n",
      "Processing      -> NNP   \n",
      "(               -> (     \n",
      "NLP             -> NNP   \n",
      ")               -> )     \n",
      "is              -> VBZ   \n",
      "a               -> DT    \n",
      "subfield        -> NN    \n",
      "of              -> IN    \n",
      "artificial      -> JJ    \n",
      "intelligence    -> NN    \n",
      ".               -> .     \n",
      "It              -> PRP   \n",
      "helps           -> VBZ   \n",
      "computers       -> NNS   \n",
      "understand      -> VBP   \n",
      ",               -> ,     \n",
      "interpret       -> JJ    \n",
      ",               -> ,     \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# NLTK POS Tagging\n",
    "nltk_pos = pos_tag(words)\n",
    "\n",
    "print(\"NLTK POS Tagging:\")\n",
    "for token, pos in nltk_pos[:20]:\n",
    "    print(f\"{token:<15} -> {pos:<6}\")\n",
    "\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "381c04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS Tagging:\n",
      "Natural         -> PROPN  (Fine-grained: NNP)\n",
      "Language        -> PROPN  (Fine-grained: NNP)\n",
      "Processing      -> PROPN  (Fine-grained: NNP)\n",
      "(               -> PUNCT  (Fine-grained: -LRB-)\n",
      "NLP             -> PROPN  (Fine-grained: NNP)\n",
      ")               -> PUNCT  (Fine-grained: -RRB-)\n",
      "is              -> AUX    (Fine-grained: VBZ)\n",
      "a               -> DET    (Fine-grained: DT)\n",
      "subfield        -> NOUN   (Fine-grained: NN)\n",
      "of              -> ADP    (Fine-grained: IN)\n",
      "artificial      -> ADJ    (Fine-grained: JJ)\n",
      "intelligence    -> NOUN   (Fine-grained: NN)\n",
      ".               -> PUNCT  (Fine-grained: .)\n",
      "It              -> PRON   (Fine-grained: PRP)\n",
      "helps           -> VERB   (Fine-grained: VBZ)\n",
      "computers       -> NOUN   (Fine-grained: NNS)\n",
      "understand      -> VERB   (Fine-grained: VB)\n",
      ",               -> PUNCT  (Fine-grained: ,)\n",
      "interpret       -> ADJ    (Fine-grained: JJ)\n",
      ",               -> PUNCT  (Fine-grained: ,)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# spaCy POS Tagging\n",
    "print(\"spaCy POS Tagging:\")\n",
    "for token in list(doc)[:20]:\n",
    "    print(f\"{token.text:<15} -> {token.pos_:<6} (Fine-grained: {token.tag_})\")\n",
    "\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426cd5f2",
   "metadata": {},
   "source": [
    "### Visualization of POS Tags with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9590379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"afcdaac16809474989c6ccbcc05ee586-0\" class=\"displacy\" width=\"1250\" height=\"257.0\" direction=\"ltr\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">Language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">Processing (</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">NLP)</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">subfield</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">artificial</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">intelligence.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,62.0 165.0,62.0 165.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-1\" stroke-width=\"2px\" d=\"M190,122.0 C190,62.0 285.0,62.0 285.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,124.0 L182,112.0 198,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-2\" stroke-width=\"2px\" d=\"M310,122.0 C310,2.0 530.0,2.0 530.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,124.0 L302,112.0 318,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-3\" stroke-width=\"2px\" d=\"M310,122.0 C310,62.0 405.0,62.0 405.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M405.0,124.0 L413.0,112.0 397.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-4\" stroke-width=\"2px\" d=\"M670,122.0 C670,62.0 765.0,62.0 765.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,124.0 L662,112.0 678,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-5\" stroke-width=\"2px\" d=\"M550,122.0 C550,2.0 770.0,2.0 770.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770.0,124.0 L778.0,112.0 762.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-6\" stroke-width=\"2px\" d=\"M790,122.0 C790,62.0 885.0,62.0 885.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M885.0,124.0 L893.0,112.0 877.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-7\" stroke-width=\"2px\" d=\"M1030,122.0 C1030,62.0 1125.0,62.0 1125.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1030,124.0 L1022,112.0 1038,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-afcdaac16809474989c6ccbcc05ee586-0-8\" stroke-width=\"2px\" d=\"M910,122.0 C910,2.0 1130.0,2.0 1130.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-afcdaac16809474989c6ccbcc05ee586-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1130.0,124.0 L1138.0,112.0 1122.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Use the first sentence for visualization\n",
    "first_sentence = list(doc.sents)[0]\n",
    "\n",
    "# Display POS tags in Jupyter notebook\n",
    "displacy.render(nlp(first_sentence.text), style=\"dep\", jupyter=True, options={\"distance\": 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b264bb",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition (NER)\n",
    "\n",
    "NER identifies and classifies named entities in text into predefined categories such as person names, organizations, locations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "890941f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Named Entity Recognition:\n",
      "(S\n",
      "  Natural/JJ\n",
      "  Language/NNP\n",
      "  Processing/NNP\n",
      "  (/(\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  subfield/NN\n",
      "  of/IN\n",
      "  artificial/JJ\n",
      "  intelligence/NN\n",
      "  ./.\n",
      "  It/PRP\n",
      "  helps/VBZ\n",
      "  computers/NNS\n",
      "  understand/VBP\n",
      "  ,/,\n",
      "  interpret/JJ\n",
      "  ,/,\n",
      "  and/CC\n",
      "  manipulate/VB\n",
      "  human/JJ\n",
      "  language/NN\n",
      "  ./.\n",
      "  The/DT\n",
      "  goal/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  is/VBZ\n",
      "  to/TO\n",
      "  bridge/VB\n",
      "  the/DT\n",
      "  gap/NN\n",
      "  between/IN\n",
      "  human/JJ\n",
      "  communication/NN\n",
      "  and/CC\n",
      "  computer/NN\n",
      "  understanding/NN\n",
      "  ./.\n",
      "  Dr./NNP\n",
      "  (PERSON Smith/NNP)\n",
      "  developed/VBD\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  algorithm/NN\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n",
      "\n",
      "Extracted Named Entities:\n",
      "NLP                  -> ORGANIZATION\n",
      "NLP                  -> ORGANIZATION\n",
      "Smith                -> PERSON\n",
      "Stanford University  -> ORGANIZATION\n",
      "California           -> GPE\n"
     ]
    }
   ],
   "source": [
    "# NLTK NER\n",
    "nltk_ner = ne_chunk(pos_tag(word_tokenize(sample_text)))\n",
    "\n",
    "print(\"NLTK Named Entity Recognition:\")\n",
    "print(nltk_ner)\n",
    "\n",
    "# Extract named entities\n",
    "named_entities = []\n",
    "for chunk in nltk_ner:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        entity = ' '.join(c[0] for c in chunk)\n",
    "        entity_type = chunk.label()\n",
    "        named_entities.append((entity, entity_type))\n",
    "\n",
    "print(\"\\nExtracted Named Entities:\")\n",
    "for entity, entity_type in named_entities:\n",
    "    print(f\"{entity:<20} -> {entity_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56f63c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Named Entity Recognition:\n",
      "Natural Language Processing -> ORG        (Companies, agencies, institutions, etc.)\n",
      "NLP                  -> ORG        (Companies, agencies, institutions, etc.)\n",
      "NLP                  -> ORG        (Companies, agencies, institutions, etc.)\n",
      "Smith                -> PERSON     (People, including fictional)\n",
      "Stanford University  -> ORG        (Companies, agencies, institutions, etc.)\n",
      "California           -> GPE        (Countries, cities, states)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Natural Language Processing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NLP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") is a subfield of artificial intelligence. It helps computers understand, interpret, and manipulate human language. The goal of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NLP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is to bridge the gap between human communication and computer understanding. Dr. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Smith\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " developed a new algorithm at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Stanford University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spaCy NER\n",
    "print(\"spaCy Named Entity Recognition:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<20} -> {ent.label_:<10} ({spacy.explain(ent.label_)})\")\n",
    "\n",
    "# Visualize NER\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f6f13",
   "metadata": {},
   "source": [
    "## Comparison of Lemmatization and Stemming\n",
    "\n",
    "Now let's compare lemmatization and stemming with at least 10 examples to understand the differences between these two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ea6740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Lemmatization vs. Stemming:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9fde9_row0_col0, #T_9fde9_row0_col1, #T_9fde9_row0_col2, #T_9fde9_row0_col3, #T_9fde9_row1_col0, #T_9fde9_row1_col1, #T_9fde9_row1_col2, #T_9fde9_row1_col3, #T_9fde9_row2_col0, #T_9fde9_row2_col1, #T_9fde9_row2_col2, #T_9fde9_row2_col3, #T_9fde9_row3_col0, #T_9fde9_row3_col1, #T_9fde9_row3_col2, #T_9fde9_row3_col3, #T_9fde9_row4_col0, #T_9fde9_row4_col1, #T_9fde9_row4_col2, #T_9fde9_row4_col3, #T_9fde9_row5_col0, #T_9fde9_row5_col1, #T_9fde9_row5_col2, #T_9fde9_row5_col3, #T_9fde9_row6_col0, #T_9fde9_row6_col1, #T_9fde9_row6_col2, #T_9fde9_row6_col3, #T_9fde9_row7_col0, #T_9fde9_row7_col1, #T_9fde9_row7_col2, #T_9fde9_row7_col3, #T_9fde9_row8_col0, #T_9fde9_row8_col1, #T_9fde9_row8_col2, #T_9fde9_row8_col3, #T_9fde9_row9_col0, #T_9fde9_row9_col1, #T_9fde9_row9_col2, #T_9fde9_row9_col3, #T_9fde9_row10_col0, #T_9fde9_row10_col1, #T_9fde9_row10_col2, #T_9fde9_row10_col3, #T_9fde9_row11_col0, #T_9fde9_row11_col1, #T_9fde9_row11_col2, #T_9fde9_row11_col3, #T_9fde9_row12_col0, #T_9fde9_row12_col1, #T_9fde9_row12_col2, #T_9fde9_row12_col3, #T_9fde9_row13_col0, #T_9fde9_row13_col1, #T_9fde9_row13_col2, #T_9fde9_row13_col3, #T_9fde9_row14_col0, #T_9fde9_row14_col1, #T_9fde9_row14_col2, #T_9fde9_row14_col3, #T_9fde9_row15_col0, #T_9fde9_row15_col1, #T_9fde9_row15_col2, #T_9fde9_row15_col3, #T_9fde9_row16_col0, #T_9fde9_row16_col1, #T_9fde9_row16_col2, #T_9fde9_row16_col3, #T_9fde9_row17_col0, #T_9fde9_row17_col1, #T_9fde9_row17_col2, #T_9fde9_row17_col3, #T_9fde9_row18_col0, #T_9fde9_row18_col1, #T_9fde9_row18_col2, #T_9fde9_row18_col3, #T_9fde9_row19_col0, #T_9fde9_row19_col1, #T_9fde9_row19_col2, #T_9fde9_row19_col3, #T_9fde9_row20_col0, #T_9fde9_row20_col1, #T_9fde9_row20_col2, #T_9fde9_row20_col3, #T_9fde9_row21_col0, #T_9fde9_row21_col1, #T_9fde9_row21_col2, #T_9fde9_row21_col3, #T_9fde9_row22_col0, #T_9fde9_row22_col1, #T_9fde9_row22_col2, #T_9fde9_row22_col3, #T_9fde9_row23_col0, #T_9fde9_row23_col1, #T_9fde9_row23_col2, #T_9fde9_row23_col3, #T_9fde9_row24_col0, #T_9fde9_row24_col1, #T_9fde9_row24_col2, #T_9fde9_row24_col3, #T_9fde9_row25_col0, #T_9fde9_row25_col1, #T_9fde9_row25_col2, #T_9fde9_row25_col3, #T_9fde9_row26_col0, #T_9fde9_row26_col1, #T_9fde9_row26_col2, #T_9fde9_row26_col3, #T_9fde9_row27_col0, #T_9fde9_row27_col1, #T_9fde9_row27_col2, #T_9fde9_row27_col3, #T_9fde9_row28_col0, #T_9fde9_row28_col1, #T_9fde9_row28_col2, #T_9fde9_row28_col3, #T_9fde9_row29_col0, #T_9fde9_row29_col1, #T_9fde9_row29_col2, #T_9fde9_row29_col3, #T_9fde9_row30_col0, #T_9fde9_row30_col1, #T_9fde9_row30_col2, #T_9fde9_row30_col3, #T_9fde9_row31_col0, #T_9fde9_row31_col1, #T_9fde9_row31_col2, #T_9fde9_row31_col3, #T_9fde9_row32_col0, #T_9fde9_row32_col1, #T_9fde9_row32_col2, #T_9fde9_row32_col3, #T_9fde9_row33_col0, #T_9fde9_row33_col1, #T_9fde9_row33_col2, #T_9fde9_row33_col3, #T_9fde9_row34_col0, #T_9fde9_row34_col1, #T_9fde9_row34_col2, #T_9fde9_row34_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9fde9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9fde9_level0_col0\" class=\"col_heading level0 col0\" >Original Word</th>\n",
       "      <th id=\"T_9fde9_level0_col1\" class=\"col_heading level0 col1\" >Lemmatization (NLTK)</th>\n",
       "      <th id=\"T_9fde9_level0_col2\" class=\"col_heading level0 col2\" >Porter Stemmer</th>\n",
       "      <th id=\"T_9fde9_level0_col3\" class=\"col_heading level0 col3\" >Lancaster Stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9fde9_row0_col0\" class=\"data row0 col0\" >running</td>\n",
       "      <td id=\"T_9fde9_row0_col1\" class=\"data row0 col1\" >run</td>\n",
       "      <td id=\"T_9fde9_row0_col2\" class=\"data row0 col2\" >run</td>\n",
       "      <td id=\"T_9fde9_row0_col3\" class=\"data row0 col3\" >run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9fde9_row1_col0\" class=\"data row1 col0\" >runs</td>\n",
       "      <td id=\"T_9fde9_row1_col1\" class=\"data row1 col1\" >run</td>\n",
       "      <td id=\"T_9fde9_row1_col2\" class=\"data row1 col2\" >run</td>\n",
       "      <td id=\"T_9fde9_row1_col3\" class=\"data row1 col3\" >run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_9fde9_row2_col0\" class=\"data row2 col0\" >ran</td>\n",
       "      <td id=\"T_9fde9_row2_col1\" class=\"data row2 col1\" >ran</td>\n",
       "      <td id=\"T_9fde9_row2_col2\" class=\"data row2 col2\" >ran</td>\n",
       "      <td id=\"T_9fde9_row2_col3\" class=\"data row2 col3\" >ran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_9fde9_row3_col0\" class=\"data row3 col0\" >better</td>\n",
       "      <td id=\"T_9fde9_row3_col1\" class=\"data row3 col1\" >well</td>\n",
       "      <td id=\"T_9fde9_row3_col2\" class=\"data row3 col2\" >better</td>\n",
       "      <td id=\"T_9fde9_row3_col3\" class=\"data row3 col3\" >bet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_9fde9_row4_col0\" class=\"data row4 col0\" >best</td>\n",
       "      <td id=\"T_9fde9_row4_col1\" class=\"data row4 col1\" >best</td>\n",
       "      <td id=\"T_9fde9_row4_col2\" class=\"data row4 col2\" >best</td>\n",
       "      <td id=\"T_9fde9_row4_col3\" class=\"data row4 col3\" >best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_9fde9_row5_col0\" class=\"data row5 col0\" >good</td>\n",
       "      <td id=\"T_9fde9_row5_col1\" class=\"data row5 col1\" >good</td>\n",
       "      <td id=\"T_9fde9_row5_col2\" class=\"data row5 col2\" >good</td>\n",
       "      <td id=\"T_9fde9_row5_col3\" class=\"data row5 col3\" >good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_9fde9_row6_col0\" class=\"data row6 col0\" >studies</td>\n",
       "      <td id=\"T_9fde9_row6_col1\" class=\"data row6 col1\" >study</td>\n",
       "      <td id=\"T_9fde9_row6_col2\" class=\"data row6 col2\" >studi</td>\n",
       "      <td id=\"T_9fde9_row6_col3\" class=\"data row6 col3\" >study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_9fde9_row7_col0\" class=\"data row7 col0\" >studying</td>\n",
       "      <td id=\"T_9fde9_row7_col1\" class=\"data row7 col1\" >study</td>\n",
       "      <td id=\"T_9fde9_row7_col2\" class=\"data row7 col2\" >studi</td>\n",
       "      <td id=\"T_9fde9_row7_col3\" class=\"data row7 col3\" >study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_9fde9_row8_col0\" class=\"data row8 col0\" >studied</td>\n",
       "      <td id=\"T_9fde9_row8_col1\" class=\"data row8 col1\" >study</td>\n",
       "      <td id=\"T_9fde9_row8_col2\" class=\"data row8 col2\" >studi</td>\n",
       "      <td id=\"T_9fde9_row8_col3\" class=\"data row8 col3\" >study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_9fde9_row9_col0\" class=\"data row9 col0\" >mice</td>\n",
       "      <td id=\"T_9fde9_row9_col1\" class=\"data row9 col1\" >mouse</td>\n",
       "      <td id=\"T_9fde9_row9_col2\" class=\"data row9 col2\" >mice</td>\n",
       "      <td id=\"T_9fde9_row9_col3\" class=\"data row9 col3\" >mic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_9fde9_row10_col0\" class=\"data row10 col0\" >mouse</td>\n",
       "      <td id=\"T_9fde9_row10_col1\" class=\"data row10 col1\" >mouse</td>\n",
       "      <td id=\"T_9fde9_row10_col2\" class=\"data row10 col2\" >mous</td>\n",
       "      <td id=\"T_9fde9_row10_col3\" class=\"data row10 col3\" >mous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_9fde9_row11_col0\" class=\"data row11 col0\" >meeting</td>\n",
       "      <td id=\"T_9fde9_row11_col1\" class=\"data row11 col1\" >meeting</td>\n",
       "      <td id=\"T_9fde9_row11_col2\" class=\"data row11 col2\" >meet</td>\n",
       "      <td id=\"T_9fde9_row11_col3\" class=\"data row11 col3\" >meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_9fde9_row12_col0\" class=\"data row12 col0\" >meet</td>\n",
       "      <td id=\"T_9fde9_row12_col1\" class=\"data row12 col1\" >meet</td>\n",
       "      <td id=\"T_9fde9_row12_col2\" class=\"data row12 col2\" >meet</td>\n",
       "      <td id=\"T_9fde9_row12_col3\" class=\"data row12 col3\" >meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_9fde9_row13_col0\" class=\"data row13 col0\" >caring</td>\n",
       "      <td id=\"T_9fde9_row13_col1\" class=\"data row13 col1\" >care</td>\n",
       "      <td id=\"T_9fde9_row13_col2\" class=\"data row13 col2\" >care</td>\n",
       "      <td id=\"T_9fde9_row13_col3\" class=\"data row13 col3\" >car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_9fde9_row14_col0\" class=\"data row14 col0\" >cares</td>\n",
       "      <td id=\"T_9fde9_row14_col1\" class=\"data row14 col1\" >care</td>\n",
       "      <td id=\"T_9fde9_row14_col2\" class=\"data row14 col2\" >care</td>\n",
       "      <td id=\"T_9fde9_row14_col3\" class=\"data row14 col3\" >car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_9fde9_row15_col0\" class=\"data row15 col0\" >cared</td>\n",
       "      <td id=\"T_9fde9_row15_col1\" class=\"data row15 col1\" >care</td>\n",
       "      <td id=\"T_9fde9_row15_col2\" class=\"data row15 col2\" >care</td>\n",
       "      <td id=\"T_9fde9_row15_col3\" class=\"data row15 col3\" >car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_9fde9_row16_col0\" class=\"data row16 col0\" >dogs</td>\n",
       "      <td id=\"T_9fde9_row16_col1\" class=\"data row16 col1\" >dog</td>\n",
       "      <td id=\"T_9fde9_row16_col2\" class=\"data row16 col2\" >dog</td>\n",
       "      <td id=\"T_9fde9_row16_col3\" class=\"data row16 col3\" >dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_9fde9_row17_col0\" class=\"data row17 col0\" >dog</td>\n",
       "      <td id=\"T_9fde9_row17_col1\" class=\"data row17 col1\" >dog</td>\n",
       "      <td id=\"T_9fde9_row17_col2\" class=\"data row17 col2\" >dog</td>\n",
       "      <td id=\"T_9fde9_row17_col3\" class=\"data row17 col3\" >dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_9fde9_row18_col0\" class=\"data row18 col0\" >wolves</td>\n",
       "      <td id=\"T_9fde9_row18_col1\" class=\"data row18 col1\" >wolf</td>\n",
       "      <td id=\"T_9fde9_row18_col2\" class=\"data row18 col2\" >wolv</td>\n",
       "      <td id=\"T_9fde9_row18_col3\" class=\"data row18 col3\" >wolv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_9fde9_row19_col0\" class=\"data row19 col0\" >wolf</td>\n",
       "      <td id=\"T_9fde9_row19_col1\" class=\"data row19 col1\" >wolf</td>\n",
       "      <td id=\"T_9fde9_row19_col2\" class=\"data row19 col2\" >wolf</td>\n",
       "      <td id=\"T_9fde9_row19_col3\" class=\"data row19 col3\" >wolf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_9fde9_row20_col0\" class=\"data row20 col0\" >walking</td>\n",
       "      <td id=\"T_9fde9_row20_col1\" class=\"data row20 col1\" >walk</td>\n",
       "      <td id=\"T_9fde9_row20_col2\" class=\"data row20 col2\" >walk</td>\n",
       "      <td id=\"T_9fde9_row20_col3\" class=\"data row20 col3\" >walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_9fde9_row21_col0\" class=\"data row21 col0\" >walked</td>\n",
       "      <td id=\"T_9fde9_row21_col1\" class=\"data row21 col1\" >walk</td>\n",
       "      <td id=\"T_9fde9_row21_col2\" class=\"data row21 col2\" >walk</td>\n",
       "      <td id=\"T_9fde9_row21_col3\" class=\"data row21 col3\" >walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_9fde9_row22_col0\" class=\"data row22 col0\" >writing</td>\n",
       "      <td id=\"T_9fde9_row22_col1\" class=\"data row22 col1\" >write</td>\n",
       "      <td id=\"T_9fde9_row22_col2\" class=\"data row22 col2\" >write</td>\n",
       "      <td id=\"T_9fde9_row22_col3\" class=\"data row22 col3\" >writ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_9fde9_row23_col0\" class=\"data row23 col0\" >wrote</td>\n",
       "      <td id=\"T_9fde9_row23_col1\" class=\"data row23 col1\" >write</td>\n",
       "      <td id=\"T_9fde9_row23_col2\" class=\"data row23 col2\" >wrote</td>\n",
       "      <td id=\"T_9fde9_row23_col3\" class=\"data row23 col3\" >wrot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_9fde9_row24_col0\" class=\"data row24 col0\" >written</td>\n",
       "      <td id=\"T_9fde9_row24_col1\" class=\"data row24 col1\" >write</td>\n",
       "      <td id=\"T_9fde9_row24_col2\" class=\"data row24 col2\" >written</td>\n",
       "      <td id=\"T_9fde9_row24_col3\" class=\"data row24 col3\" >writ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_9fde9_row25_col0\" class=\"data row25 col0\" >worse</td>\n",
       "      <td id=\"T_9fde9_row25_col1\" class=\"data row25 col1\" >bad</td>\n",
       "      <td id=\"T_9fde9_row25_col2\" class=\"data row25 col2\" >wors</td>\n",
       "      <td id=\"T_9fde9_row25_col3\" class=\"data row25 col3\" >wors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_9fde9_row26_col0\" class=\"data row26 col0\" >worst</td>\n",
       "      <td id=\"T_9fde9_row26_col1\" class=\"data row26 col1\" >bad</td>\n",
       "      <td id=\"T_9fde9_row26_col2\" class=\"data row26 col2\" >worst</td>\n",
       "      <td id=\"T_9fde9_row26_col3\" class=\"data row26 col3\" >worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_9fde9_row27_col0\" class=\"data row27 col0\" >bad</td>\n",
       "      <td id=\"T_9fde9_row27_col1\" class=\"data row27 col1\" >bad</td>\n",
       "      <td id=\"T_9fde9_row27_col2\" class=\"data row27 col2\" >bad</td>\n",
       "      <td id=\"T_9fde9_row27_col3\" class=\"data row27 col3\" >bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_9fde9_row28_col0\" class=\"data row28 col0\" >singing</td>\n",
       "      <td id=\"T_9fde9_row28_col1\" class=\"data row28 col1\" >sing</td>\n",
       "      <td id=\"T_9fde9_row28_col2\" class=\"data row28 col2\" >sing</td>\n",
       "      <td id=\"T_9fde9_row28_col3\" class=\"data row28 col3\" >sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_9fde9_row29_col0\" class=\"data row29 col0\" >sang</td>\n",
       "      <td id=\"T_9fde9_row29_col1\" class=\"data row29 col1\" >sang</td>\n",
       "      <td id=\"T_9fde9_row29_col2\" class=\"data row29 col2\" >sang</td>\n",
       "      <td id=\"T_9fde9_row29_col3\" class=\"data row29 col3\" >sang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_9fde9_row30_col0\" class=\"data row30 col0\" >sung</td>\n",
       "      <td id=\"T_9fde9_row30_col1\" class=\"data row30 col1\" >sung</td>\n",
       "      <td id=\"T_9fde9_row30_col2\" class=\"data row30 col2\" >sung</td>\n",
       "      <td id=\"T_9fde9_row30_col3\" class=\"data row30 col3\" >sung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_9fde9_row31_col0\" class=\"data row31 col0\" >easily</td>\n",
       "      <td id=\"T_9fde9_row31_col1\" class=\"data row31 col1\" >easily</td>\n",
       "      <td id=\"T_9fde9_row31_col2\" class=\"data row31 col2\" >easili</td>\n",
       "      <td id=\"T_9fde9_row31_col3\" class=\"data row31 col3\" >easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_9fde9_row32_col0\" class=\"data row32 col0\" >easy</td>\n",
       "      <td id=\"T_9fde9_row32_col1\" class=\"data row32 col1\" >easy</td>\n",
       "      <td id=\"T_9fde9_row32_col2\" class=\"data row32 col2\" >easi</td>\n",
       "      <td id=\"T_9fde9_row32_col3\" class=\"data row32 col3\" >easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_9fde9_row33_col0\" class=\"data row33 col0\" >happiness</td>\n",
       "      <td id=\"T_9fde9_row33_col1\" class=\"data row33 col1\" >happiness</td>\n",
       "      <td id=\"T_9fde9_row33_col2\" class=\"data row33 col2\" >happi</td>\n",
       "      <td id=\"T_9fde9_row33_col3\" class=\"data row33 col3\" >happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9fde9_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_9fde9_row34_col0\" class=\"data row34 col0\" >happy</td>\n",
       "      <td id=\"T_9fde9_row34_col1\" class=\"data row34 col1\" >happy</td>\n",
       "      <td id=\"T_9fde9_row34_col2\" class=\"data row34 col2\" >happi</td>\n",
       "      <td id=\"T_9fde9_row34_col3\" class=\"data row34 col3\" >happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1bc94828d60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a list of example words for comparison\n",
    "example_words = [\n",
    "    'running', 'runs', 'ran',          # Run variations\n",
    "    'better', 'best', 'good',          # Good variations\n",
    "    'studies', 'studying', 'studied',  # Study variations\n",
    "    'mice', 'mouse',                   # Irregular plurals\n",
    "    'meeting', 'meet',                 # Meet variations\n",
    "    'caring', 'cares', 'cared',        # Care variations\n",
    "    'dogs', 'dog',                     # Regular plurals\n",
    "    'wolves', 'wolf',                  # Irregular plurals\n",
    "    'walking', 'walked',               # Walk variations\n",
    "    'writing', 'wrote', 'written',     # Write variations\n",
    "    'worse', 'worst', 'bad',           # Bad variations\n",
    "    'singing', 'sang', 'sung',         # Sing variations\n",
    "    'easily', 'easy',                  # Adverb formation\n",
    "    'happiness', 'happy'               # Noun from adjective\n",
    "]\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag([word])[0][1])) for word in example_words]\n",
    "porter_stems = [porter_stemmer.stem(word) for word in example_words]\n",
    "lancaster_stems = [lancaster_stemmer.stem(word) for word in example_words]\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Word': example_words,\n",
    "    'Lemmatization (NLTK)': lemmas,\n",
    "    'Porter Stemmer': porter_stems,\n",
    "    'Lancaster Stemmer': lancaster_stems\n",
    "})\n",
    "\n",
    "# Display the comparison\n",
    "print(\"Comparison of Lemmatization vs. Stemming:\")\n",
    "display(comparison_df.style.set_properties(**{'text-align': 'left'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be9d01",
   "metadata": {},
   "source": [
    "### Key Differences Between Lemmatization and Stemming:\n",
    "\n",
    "1. **Approach**:\n",
    "   - **Stemming**: Uses heuristic rules to chop off word endings, often resulting in non-dictionary words.\n",
    "   - **Lemmatization**: Analyzes word structure and uses morphological analysis to return proper dictionary forms.\n",
    "\n",
    "2. **Part-of-Speech Awareness**:\n",
    "   - **Stemming**: Typically doesn't consider the part of speech of the word.\n",
    "   - **Lemmatization**: Usually takes into account the part of speech to apply the correct normalization rules.\n",
    "\n",
    "3. **Output Validity**:\n",
    "   - **Stemming**: May produce stems that are not actual words (e.g., 'runn', 'studi').\n",
    "   - **Lemmatization**: Produces valid dictionary words (lemmas) (e.g., 'run', 'study').\n",
    "\n",
    "4. **Handling of Irregular Forms**:\n",
    "   - **Stemming**: Generally fails with irregular forms (e.g., 'better' → 'better', not 'good').\n",
    "   - **Lemmatization**: Properly handles irregular forms (e.g., 'better' → 'good', 'mice' → 'mouse').\n",
    "\n",
    "5. **Accuracy vs. Speed**:\n",
    "   - **Stemming**: Faster but less accurate.\n",
    "   - **Lemmatization**: More accurate but computationally more intensive.\n",
    "\n",
    "6. **Use Cases**:\n",
    "   - **Stemming**: Better for search engines and information retrieval where exact form is less important.\n",
    "   - **Lemmatization**: Better for text analysis, NLP tasks where meaning and linguistic correctness matter.\n",
    "\n",
    "7. **Dictionary Dependency**:\n",
    "   - **Stemming**: Rule-based, doesn't require a dictionary.\n",
    "   - **Lemmatization**: Often requires a dictionary lookup to determine the lemma.\n",
    "\n",
    "The examples above clearly demonstrate these differences. Notice how stemming sometimes produces unusual forms ('happili', 'wors'), while lemmatization consistently returns valid words but may miss some connections (treating 'worse/worst/bad' as separate words unless provided with context)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54c18c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have implemented and explored various NLP preprocessing techniques:\n",
    "\n",
    "1. **Tokenization**: Breaking text into words and sentences using both NLTK and spaCy.\n",
    "2. **Lemmatization**: Reducing words to their base dictionary forms, considering context and part of speech.\n",
    "3. **Stemming**: Reducing words to their word stems by removing affixes.\n",
    "4. **POS Tagging**: Identifying the grammatical parts of speech for each token.\n",
    "5. **Named Entity Recognition (NER)**: Identifying and classifying named entities in text.\n",
    "\n",
    "We've also compared lemmatization and stemming using multiple examples, highlighting the key differences between these two text normalization approaches. While stemming is faster and simpler, lemmatization provides more linguistically accurate results, especially for irregular word forms and when maintaining meaningful word representations is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4beca5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
