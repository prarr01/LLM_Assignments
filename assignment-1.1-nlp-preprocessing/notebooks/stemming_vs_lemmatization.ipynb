 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming vs Lemmatization in NLP\n",
    "\n",
    "This notebook demonstrates the differences between stemming and lemmatization in Natural Language Processing, using NLTK and spaCy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Stemming with Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Example words to stem\n",
    "words = ['running', 'runs', 'ran', 'runner', 'easily', 'fairly', 'fairness']\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [porter.stem(word) for word in words]\n",
    "\n",
    "# Display results\n",
    "for original, stemmed in zip(words, stemmed_words):\n",
    "    print(f\"{original} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lemmatization with WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words to lemmatize\n",
    "words = ['running', 'runs', 'ran', 'runner', 'easily', 'fairly', 'fairness']\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Display results\n",
    "for original, lemmatized in zip(words, lemmatized_words):\n",
    "    print(f\"{original} -> {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example text\n",
    "text = \"The running man runs easily and fairly. The runner ran yesterday.\"\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Display lemmatization results\n",
    "print(\"spaCy Lemmatization:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-world Example with Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example sentence\n",
    "sentence = \"The cats are running and jumping in the garden while the dogs are barking loudly.\"\n",
    "\n",
    "# Tokenize sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Display results\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"\\nStemmed tokens:\", stemmed_tokens)\n",
    "print(\"\\nLemmatized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# Create a larger text for testing\n",
    "test_text = \"The running man runs easily and fairly. The runner ran yesterday. \" * 100\n",
    "tokens = word_tokenize(test_text)\n",
    "\n",
    "# Test stemming performance\n",
    "start_time = time.time()\n",
    "stemmed = [porter.stem(token) for token in tokens]\n",
    "stemming_time = time.time() - start_time\n",
    "\n",
    "# Test lemmatization performance\n",
    "start_time = time.time()\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "lemmatization_time = time.time() - start_time\n",
    "\n",
    "print(f\"Stemming time: {stemming_time:.4f} seconds\")\n",
    "print(f\"Lemmatization time: {lemmatization_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. When to Use Each Method\n",
    "\n",
    "### Stemming\n",
    "- Faster processing\n",
    "- Good for information retrieval\n",
    "- Less accurate but more aggressive\n",
    "- Useful for search engines\n",
    "\n",
    "### Lemmatization\n",
    "- More accurate results\n",
    "- Preserves meaning\n",
    "- Slower processing\n",
    "- Better for text analysis and understanding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
